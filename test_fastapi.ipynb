{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992714c-a3ee-4c85-8413-a89774dedb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyngrok nest_asyncio fastapi uvicorn loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32d039-2d06-4440-9f41-52a725156212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working!!!\n",
    "import asyncio\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "# 1. Patch the loop to allow nested runs\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# 2. Add CORS Middleware (CRITICAL for Electron)\n",
    "# This allows your Electron app to talk to the local server without security blocks\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"], # In production, replace with your electron origin\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class UserRequestIn(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def hello():\n",
    "    return {\"message\": \"Hello World\", \"status\": \"Ready for Electron\"}\n",
    "\n",
    "@app.get(\"/hi\")\n",
    "async def hello2():\n",
    "    return {\"message\": \"Hi from Jupyter\", \"status\": \"Juypter Result\"}\n",
    "\n",
    "@app.post(\"/test\")\n",
    "async def index(request: UserRequestIn):\n",
    "    logger.debug(f\"Payload received: {request.text}\")\n",
    "    # Here you would call your HF model logic:\n",
    "    # result = my_hf_pipeline(request.text)\n",
    "    return {\"ok\": True, \"processed_text\": request.text.upper()}\n",
    "\n",
    "# 3. Start Server in the Background\n",
    "config = uvicorn.Config(app, host=\"127.0.0.1\", port=8000, loop=\"asyncio\")\n",
    "server = uvicorn.Server(config)\n",
    "\n",
    "# Instead of uvicorn.run(), we create a task in the existing Jupyter loop\n",
    "loop = asyncio.get_event_loop()\n",
    "if not any(\"serve\" in str(task) for task in asyncio.all_tasks()):\n",
    "    loop.create_task(server.serve())\n",
    "    print(\"‚úÖ Server is running in background at http://127.0.0.1:8000\")\n",
    "    print(\"Check 'Hello World' at http://127.0.0.1:8000/\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Server task is already running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c5e5a-4850-4ad9-b35d-eb19e4c72bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# 1. Configuration\n",
    "NGROK_AUTH_TOKEN = \"38L4BHWpv8dFLtHUSfV5hyYOVMK_73VHZbQJ2RoPg9Nh8WAgS\" # Get from dashboard.ngrok.com\n",
    "PORT = 8000\n",
    "\n",
    "# 2. Setup FastAPI & CORS\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class UserRequestIn(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def hello():\n",
    "    return {\"message\": \"Hello World\", \"status\": \"Online\", \"mode\": \"Public via ngrok\"}\n",
    "\n",
    "@app.get(\"/hi\")\n",
    "async def hello2():\n",
    "    return {\"message\": \"Hi from Jupyter\", \"status\": \"Juypter Result\"}\n",
    "\n",
    "@app.post(\"/test\")\n",
    "async def index(request: UserRequestIn):\n",
    "    logger.info(f\"Incoming request: {request.text[:50]}...\")\n",
    "    # Your model logic goes here\n",
    "    return {\"ok\": True, \"received\": request.text}\n",
    "\n",
    "# 3. Start Server & Tunnel\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def start_services():\n",
    "    # Setup Ngrok\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    \n",
    "    # Check if tunnel already exists to avoid duplicates\n",
    "    tunnels = ngrok.get_tunnels()\n",
    "    if not tunnels:\n",
    "        public_url = ngrok.connect(PORT).public_url\n",
    "        print(f\"üöÄ Public URL: {public_url}\")\n",
    "    else:\n",
    "        print(f\"üöÄ Public URL: {tunnels[0].public_url}\")\n",
    "\n",
    "    # Setup Uvicorn\n",
    "    config = uvicorn.Config(app, host=\"127.0.0.1\", port=PORT, loop=\"asyncio\")\n",
    "    server = uvicorn.Server(config)\n",
    "    \n",
    "    # Run server in the background\n",
    "    if not any(\"serve\" in str(task) for task in asyncio.all_tasks()):\n",
    "        asyncio.create_task(server.serve())\n",
    "        print(f\"üè† Local URL: http://127.0.0.1:{PORT}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Server is already running.\")\n",
    "\n",
    "# Execute\n",
    "asyncio.run(start_services())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298329ac-0266-41fe-8efc-051801861497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class UserRequestIn(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "def hello_world():\n",
    "    return {\"message\": \"Hello World\", \"status\": \"AI Server is online\"}\n",
    "    \n",
    "@app.post(\"/test\")\n",
    "def index(request: UserRequestIn):\n",
    "    logger.debug(request)\n",
    "    return {\"ok\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4d135-fd37-437d-8f6f-a44256b75802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Replace 'YOUR_AUTHTOKEN_HERE' with the token from the dashboard\n",
    "NGROK_AUTH_TOKEN = \"38L4BHWpv8dFLtHUSfV5hyYOVMK_73VHZbQJ2RoPg9Nh8WAgS\"\n",
    "\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d959094-e578-42ea-84d4-82e2baa86d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyngrok import ngrok\n",
    "\n",
    "#ngrok_tunnel = ngrok.connect(8000)\n",
    "\n",
    "#ngrok_tunnel\n",
    "# Open a HTTP tunnel on port 8000\n",
    "public_url = ngrok.connect(8000).public_url\n",
    "print(f\"Electron can now access the models at: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3d56a-ab03-4756-b697-b711ac3a16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f6d7d-2545-4684-98a7-f91973cae4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f735f4-6354-4b4d-9e4c-c210e6419ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q\n",
    "!pip install datasets -q\n",
    "!pip install spacy -q\n",
    "!pip install wordcloud -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51274e40-8d46-4d21-82cb-9285a275231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP & Text Preprocessing\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn: Data Handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scikit-learn: Text Processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Scikit-learn: Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Scikit-learn: Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6459f-5794-40ef-9312-d2bb9391e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kagglehub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27c505-a245-4b1f-accd-3fcd3724415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "email_path = kagglehub.dataset_download(\"naserabdullahalam/phishing-email-dataset\")\n",
    "\n",
    "print(\"Path to Email dataset files:\", email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906ebda-67f9-4327-9c92-b577b4d9368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all files in the dataset folder\n",
    "email_files = os.listdir(email_path)\n",
    "\n",
    "print(\"Files in Email dataset:\", email_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d97ede-fd3b-4162-a9fc-5023c94161b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load all CSV files\n",
    "email_df1 = pd.read_csv(f'{email_path}/CEAS_08.csv')\n",
    "email_df2 = pd.read_csv(f'{email_path}/Enron.csv')\n",
    "email_df3 = pd.read_csv(f'{email_path}/Ling.csv')\n",
    "email_df4 = pd.read_csv(f'{email_path}/Nazario.csv')\n",
    "email_df5 = pd.read_csv(f'{email_path}/Nigerian_Fraud.csv')\n",
    "email_df6 = pd.read_csv(f'{email_path}/SpamAssasin.csv')\n",
    "email_df7 = pd.read_csv(f'{email_path}/phishing_email.csv')\n",
    "#print(email_df.head())\n",
    "\n",
    "# 3. Rename the column in the specific dataframe\n",
    "# This changes 'text_combined' to 'body' so it matches common schemas\n",
    "email_df7 = email_df7.rename(columns={'text_combined': 'body'})\n",
    "\n",
    "# 4. Merge all dataframes into one\n",
    "# We use ignore_index=True to create a fresh index for the combined set\n",
    "all_emails_df = pd.concat([\n",
    "    email_df1, email_df2, email_df3, \n",
    "    email_df4, email_df5, email_df6, email_df7\n",
    "], ignore_index=True)\n",
    "\n",
    "# 5. Verify the results\n",
    "print(f\"Total rows in combined dataset: {len(all_emails_df)}\")\n",
    "print(all_emails_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3563c-c6f2-48b7-9a41-15bc36fc10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d3f7f-4385-4964-9427-ee18daabf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a115b-dced-412b-97b1-07b22e567753",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc40202-2393-4690-9991-268ca272cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfac6b-920b-4848-acbd-e0849eaf738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df['email'] = all_emails_df['subject'] + ' ' + all_emails_df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67feac7-2e3d-4c50-8b0e-8f2b7705ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unused columns\n",
    "all_emails_df = all_emails_df.drop(columns=['subject', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11af6ae-0a99-4fd6-b02a-a8a736fed9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252293f0-e1ed-4d7d-8701-adc814343b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3433b29b-c0ef-49aa-8006-cd3829cafa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ef516-336b-4e55-b196-bf3fb56b6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40f4e1-3e2e-4c90-8eb8-71f7bcce9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a52ff-a65e-47ea-9077-37a43deb0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6974d1f-560d-42ef-b60b-25c4d18e6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_email(text):\n",
    "    # Handle Empty values\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    text = soup.get_text(separator=' ')\n",
    "\n",
    "    # Replace Links with [URL]\n",
    "    url_pattern = r'(https?://\\S+|www\\.\\S+)'\n",
    "    text = re.sub(url_pattern, '[URL]', text)\n",
    "\n",
    "    # 4. Remove Email Addresses\n",
    "    email_pattern = r'\\S+@\\S+'\n",
    "    text = re.sub(email_pattern, '[EMAIL]', text)\n",
    "\n",
    "    # Process with spaCy\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    # Remove stopwords + non-alphabetic + lemmatize\n",
    "    tokens = [\n",
    "      token.lemma_\n",
    "      for token in doc\n",
    "      if (token.is_alpha or token.text in ['[URL]', '[EMAIL]']) and not token.is_stop\n",
    "    ]\n",
    "\n",
    "    # Join tokens back to string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "all_emails_df['cleaned_email'] = all_emails_df['email'].apply(preprocess_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3abbf3-a87e-48ef-9163-01875cb380ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4601a5-15a5-4351-9fa7-73394a1a01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the most frequent words per category using bar plots\n",
    "real_emails = ' '.join(all_emails_df[all_emails_df['label']==0]['cleaned_email'])\n",
    "spam_emails = ' '.join(all_emails_df[all_emails_df['label']==1]['cleaned_email'])\n",
    "\n",
    "def most_common_words(text, title, n, filename):\n",
    "    if not text:\n",
    "        print(f'No words found for {title}')\n",
    "        return\n",
    "\n",
    "    words = text.split()  # Split the text into words\n",
    "    counter = Counter(words)\n",
    "    common = counter.most_common(n) # Get the top n most common words\n",
    "\n",
    "    if not common:\n",
    "        print(f'No words found for {title}')\n",
    "        return\n",
    "\n",
    "    words, counts = zip(*common)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(words, counts, color='#33f')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # ‚úÖ Save as image\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "most_common_words(real_emails, 'Top Real Words', 20, 'top_real_Email_Words.png')\n",
    "most_common_words(spam_emails, 'Top Spam Words', 20, 'top_spam_Email_Words.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f08373-2a3b-4ea8-b5be-76f6042044b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the most frequent words per category using word clouds\n",
    "real_WordCloud = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(real_emails)\n",
    "spam_WordCloud = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(spam_emails)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(real_WordCloud, interpolation='bilinear')\n",
    "plt.title('Most frequent Real Email Words')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(spam_WordCloud, interpolation='bilinear')\n",
    "plt.title('Most frequent Spam Words')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ‚úÖ Save as image\n",
    "plt.savefig('word_cloud.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd849519-3d61-4c99-9415-8c8949f28365",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_emails_df.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9ac02-3e55-4800-917a-76ddd9fea34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion 01\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train-Test Split\n",
    "X = data['cleaned_email']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23, stratify=y)\n",
    "\n",
    "# Build Pipelines with Multiple Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(solver='saga', max_iter=1000, random_state=23),\n",
    "    'Linear SVM': LinearSVC(random_state=23),\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=23),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=23)\n",
    "}\n",
    "\n",
    "result = {}\n",
    "\n",
    "for name, regressor in models.items():\n",
    "    # Note: Using Pipeline to ensure vectorization is fit only on training data\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', TfidfVectorizer(ngram_range=(1,2), max_df=0.8, min_df=5)),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    result[name] = accuracy\n",
    "\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Classification Report:\\n{classification_report(y_test, y_pred)}\\n')\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15b176-c484-4946-aa33-69839b31f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion 02\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train-Test Split\n",
    "X = data['cleaned_email']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23, stratify=y)\n",
    "\n",
    "# Expanded Model Dictionary\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(solver='saga', max_iter=1000, random_state=23),\n",
    "    'Linear SVM': LinearSVC(random_state=23),\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=23),\n",
    "    'SGD Classifier': SGDClassifier(loss='hinge', penalty='l2', random_state=23),\n",
    "    'Passive Aggressive': PassiveAggressiveClassifier(max_iter=1000, random_state=23),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=23)\n",
    "}\n",
    "\n",
    "result = {}\n",
    "\n",
    "for name, regressor in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', TfidfVectorizer(ngram_range=(1,2), max_df=0.8, min_df=5)),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    # Train and evaluate\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    result[name] = accuracy\n",
    "\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Classification Report\\n {classification_report(y_test, y_pred)}\\n')\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361b400-192e-41ca-90ba-00e58241cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansion 03\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train-Test Split\n",
    "X = data['cleaned_email']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23, stratify=y)\n",
    "\n",
    "# Expanded Model Dictionary\n",
    "models = {\n",
    "    # Linear & Fast Models\n",
    "    'Logistic Regression': LogisticRegression(solver='saga', max_iter=1000, random_state=23),\n",
    "    'Linear SVM': LinearSVC(random_state=23),\n",
    "    'Ridge Classifier': RidgeClassifier(random_state=23),\n",
    "    'SGD Classifier': SGDClassifier(loss='modified_huber', random_state=23), # modified_huber gives probas\n",
    "    \n",
    "    # Naive Bayes (Standard for Text)\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Complement NB': ComplementNB(), # Better for imbalanced text data\n",
    "    \n",
    "    # Tree Ensembles\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=23),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, n_jobs=-1, random_state=23),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=23),\n",
    "    \n",
    "    # Gradient Boosting (State-of-the-art)\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=23),\n",
    "    'LightGBM': LGBMClassifier(random_state=23, verbose=-1)\n",
    "}\n",
    "\n",
    "result = {}\n",
    "\n",
    "for name, regressor in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', TfidfVectorizer(ngram_range=(1,2), max_df=0.8, min_df=5)),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    # Train and evaluate\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    result[name] = accuracy\n",
    "\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Classification Report:\\n {classification_report(y_test, y_pred)}\\n')\n",
    "    print('=' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f11eb7-515e-4b2c-a6be-a177ab6aac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Expanded\n",
    "# Train-Test Split\n",
    "X = data['cleaned_email']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23, stratify=y)\n",
    "\n",
    "# Build Pipelines with Multiple Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(solver='saga', max_iter=1000, random_state=23),\n",
    "    'SVM': LinearSVC(random_state=23)\n",
    "}\n",
    "\n",
    "result = {}\n",
    "\n",
    "for name, regressor in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', TfidfVectorizer(ngram_range=(1,2), max_df=0.8, min_df=5)),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    result[name] = accuracy\n",
    "\n",
    "    print(f'Model: {name}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Classification Report\\n {classification_report(y_test, y_pred)}\\n')\n",
    "    print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39fc01-19a4-48eb-954a-4b857b72a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "best_model = max(result, key=result.get)\n",
    "best_accuracy = result[best_model]\n",
    "print(f'The best model is {best_model}, with an accuracy of {best_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cf310-31be-4f66-b235-5873e22b9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "for name, regressor in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', TfidfVectorizer(ngram_range=(1,2), max_df=0.8, min_df=5)),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "# Train and evaluate the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "    ConfusionMatrixDisplay(cm, display_labels=['Spam Emails', 'Real Emails']).plot(cmap='Blues', ax=ax)\n",
    "    plt.title(f'Confusion Matrix: {name}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ‚úÖ Save as image\n",
    "    plt.savefig(f'confusion_matrix_{name.replace(\" \", \"_\").lower()}.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1edbd-f11d-44b3-b476-affbcd3f5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Model Accuracies\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=result.values(), y=result.keys(), color='#33f')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.tight_layout()\n",
    "\n",
    "# ‚úÖ Save as image\n",
    "plt.savefig('compare_model_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7bd0e-18fd-4b81-8b9b-10d7a733226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on real data\n",
    "sample_emails=[\n",
    "    'CONGRATULATIONS! Your email address has been selected as the winner of the $1,000,000 Microsoft Promotion. To claim your prize, reply with your bank details immediately.',\n",
    "    'Hi Team, please find the minutes of our last meeting attached. We need to finalize the project budget by Friday. Let\\'s meet on Zoom at 2 PM to discuss.',\n",
    "    'webcam dating is hot - - - - - - - - - - - - - - - - please no more'\n",
    "]\n",
    "\n",
    "cleaned_samples = [preprocess_email(email) for email in sample_emails]\n",
    "\n",
    "best_model = Pipeline([\n",
    "    ('preprocessor', TfidfVectorizer()),\n",
    "    ('regressor', LinearSVC(random_state=23))\n",
    "])\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "preds = best_model.predict(cleaned_samples)\n",
    "\n",
    "for i, (j, k) in enumerate(zip(sample_emails, preds)):\n",
    "    label = \"SPAM üö®\" if k == 1 else \"REAL ‚úÖ\"\n",
    "    print(f'\\n{i+1}. Email: {j}')\n",
    "    print(f'Prediction: {label} (Class {k})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360af47-b7b2-4a80-9dbc-f4da2c45e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "from joblib import dump\n",
    "dump(pipeline, 'model.joblib')\n",
    "\n",
    "print('‚úÖ Pipeline trained and saved as model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
